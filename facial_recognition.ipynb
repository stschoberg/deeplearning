{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "facial_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOCPhqHq6OQ6aG0VWXkaWS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stschoberg/deeplearning/blob/main/facial_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_L2KOEQyUGD"
      },
      "source": [
        "# Facial Recognition Using MTCNN, FaceNet, and K-Means Clustering\n",
        "\n",
        "This notebook uses a combination of transfer learning with CNNs and classical machine learning algorithms to groups faces of the same identity together. Given a dataset of images, we want to be able to cluster individuals together across the entire set. In other words, we want to be able to say with a high degree of accuracy that person A is present in picture 1, 32, 53, 34, and 87. Its important to note early on that this is an unsupervised learning problem. The algorithm must be able to group faces with the same identity together without any labels. \n",
        "\n",
        "Below is a brief overview of the process needed to accomplish this.\n",
        "\n",
        "\n",
        "\n",
        "1.   Determine the location of faces in all photos from the dataset.\n",
        "2.   Using the coordinates of each face, extract those faces from the original photos.\n",
        "3. Create an embedding of that face (a feature vector) that is unique to that face. \n",
        "4. Cluster those feature vectors together to create groups of the same person. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDjRpkNz5cys"
      },
      "source": [
        "!pip3 install mtcnn\n",
        "!pip3 install opencv-contrib-python\n",
        "!pip3 install pillow\n",
        "!pip install git+git://github.com/PnS2019/pnslib.git\n",
        "!pip3 install deepface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRYdum0Xx61l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1f946c-bc70-48e3-cb29-bce22dbc6ab8"
      },
      "source": [
        "from google.colab import drive # Import photos from Google Photos\n",
        "from mtcnn.mtcnn import MTCNN # Facial detection\n",
        "from keras.models import load_model # Load pretrained models into tf (transfer learning)\n",
        "from PIL import Image # Image manipulation package\n",
        "import numpy as np\n",
        "from pnslib import utils # Download certain feature detection models (eyes, mouths)\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAoxsPPCzN4k",
        "outputId": "59128ec9-7f5b-42a0-b53c-9f2ba351a0d4"
      },
      "source": [
        "facenet = load_model('/content/gdrive/MyDrive/cv/facenet_keras.h5')\n",
        "print(facenet.inputs)\n",
        "print(facenet.outputs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "[<KerasTensor: shape=(None, 160, 160, 3) dtype=float32 (created by layer 'input_1')>]\n",
            "[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Bottleneck_BatchNorm')>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDpiAPUoG6Qy"
      },
      "source": [
        "# Takes in a filepath, opens the image, and returns the pixels as an np array\n",
        "def open_prepare_image(file):\n",
        "  image = Image.open(file).convert('RGB')\n",
        "  pixels = np.asarray(image)\n",
        "\n",
        "  return pixels"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVbqBOmmB2uz"
      },
      "source": [
        "# Reads in pxl array, outputs bounding box of all detected faces [x, y, width, height]\n",
        "def get_face_boxes(pxls):\n",
        "  detector = MTCNN()\n",
        "  results = detector.detect_faces(pxls)\n",
        "  confident = filter(lambda res: res['confidence'] > 0.95, results)\n",
        "  return [box_coords['box'] for box_coords in confident]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKsOpMYFHKR7"
      },
      "source": [
        "# Converts bounding box coords [x, y, width, height] to two points\n",
        "def calc_faces_coords(face_boxes):\n",
        "  face_coords = []\n",
        "\n",
        "  for face in face_boxes:\n",
        "    x1, y1, width, height = face\n",
        "    x1, y1 = abs(x1), abs(y1)\n",
        "    x2, y2 = x1 + width, y1 + height\n",
        "    face_coords.append([x1, y1, x2, y2])\n",
        "\n",
        "  return face_coords\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfca3jd2E9-7"
      },
      "source": [
        "# Given the coordinates of a face and pxls, extracts the face from the image and resizes it accordingly\n",
        "def faces_from_coords(face_coords, pxls, required_size=(160,160)):\n",
        "  faces = []\n",
        "  for coord in face_coords:\n",
        "    x1, y1, x2, y2 = coord\n",
        "    face = pxls[y1:y2, x1:x2]\n",
        "    image = Image.fromarray(face)\n",
        "    image = image.resize(required_size)\n",
        "    faces.append(np.asarray(image))\n",
        "\n",
        "  return faces"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8beQv-SkEH0S"
      },
      "source": [
        "y = np.array([\n",
        "     'alec', 'julia', 'erin', 'u', 'u',\n",
        "     'u', 'u', 'u', 'u', 'u',\n",
        "     'u', 'ross', 'ben', 'alec', 'ben',\n",
        "     'erin', 'julia', 'pat', 'tony', 'sam',\n",
        "     'u', 'collin', 'u', 'u', 'u',\n",
        "     'u', 'u', 'alec', 'ben', 'julia',\n",
        "     'collin', 'erin', 'tony', 'u', 'pat',\n",
        "     'u', 'u', 'sam', 'u', 'u',\n",
        "     'u', 'alec', 'tony', 'pat', 'ben',\n",
        "     'erin', 'u', 'sam', 'u', 'collin',\n",
        "     'julia', 'u', 'u', 'u', 'u',\n",
        "     'sam', 'erin', 'alec', 'remi', 'sam',\n",
        "     'alec', 'erin', 'tony', 'julia', 'max',\n",
        "     'sam', 'collin', 'sam', 'tony', 'erin',\n",
        "     'julia', 'alec', 'max', 'u', 'u',\n",
        "     'alec', 'sam', 'sam', 'sam', 'amy',\n",
        "     'collin', 'sam', 'max', 'alec', 'u',\n",
        "     'erin', 'collin', 'sam', 'max', 'u',\n",
        "     'alec', 'u', 'sam', 'erin', 'u',\n",
        "     'max', 'max', 'sam', 'max', 'u',\n",
        "     'erin', 'sam', 'jack', 'max', 'u',\n",
        "     'sam', 'benny', 'erin', 'max', 'sam',\n",
        "     'jack', 'max', 'sam', 'max', 'u',\n",
        "     'dillon', 'ross', 'max', 'dillon', 'ross',\n",
        "     'sam', 'sam', 'jack', 'max', 'ross',\n",
        "     'max', 'sam', 'jack', 'max', 'u',\n",
        "     'ross', 'max', 'sam', 'ross', 'ross',\n",
        "     'u', 'alec', 'ross', 'sam', 'pat',\n",
        "     'sam', 'benny', 'u', 'julia', 'sam',\n",
        "     'sam', 'benny', 'julia', 'benny', 'sam',\n",
        "     'julia', 'pat', 'sam', 'benny', 'julia',\n",
        "     'sam', 'benny', 'alec', 'max', 'erin',\n",
        "     'collin', 'phillipe', 'ehaab', 'sam', 'ross',\n",
        "     'julia', 'pat', 'u', 'tony', 'tony',\n",
        "     'u', 'benny', 'collin', 'ross', 'sam',\n",
        "     'max', 'phillipe', 'erin', 'ehaab', 'alec',\n",
        "     'pat', 'julia', 'jack', 'tony', 'u',\n",
        "     'collin', 'phillipe', 'alec', 'ehaab', 'max',\n",
        "     'julia', 'benny', 'erin', 'sam', 'ross',\n",
        "     'pat', 'tony', 'jack', 'max', 'sam',\n",
        "     'collin', 'alec', 'erin', 'phillipe', 'ehaab',\n",
        "     'ross', 'julia', 'sam', 'max', 'benny',\n",
        "     'collin', 'pat', 'u', 'tony', 'sam', \n",
        "     'erin', 'max', 'sam', 'alec', 'ehaab',\n",
        "     'collin', 'ross', 'benny', 'julia', 'phillipe',\n",
        "     'pat', 'u', 'tony', 'jack', 'sam',\n",
        "     'collin', 'erin', 'max', 'sam', 'alec',\n",
        "     'ehaab', 'collin', 'ross', 'benny', 'julia',\n",
        "     'phillipe', 'pat', 'u', 'tony', 'jack',\n",
        "     'sam', 'collin', 'erin', 'ehaab', 'max',\n",
        "     'benny', 'sam', 'ehaab', 'alec', 'pat',\n",
        "     'tony', 'ross', 'jack', 'julia', 'collin',\n",
        "     'u', 'ehaab', 'alec', 'sam', 'ross',\n",
        "     'max', 'erin', 'pat', 'julia', 'jack',\n",
        "     'tony', 'benny', 'jack', 'u', 'u'])\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_aa0cJO2azQ"
      },
      "source": [
        "This block below extracts all the faces from the provided images. It stores faces as a pixel array in *faces* and the bounding box for each face in *face_boxes_all*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZd1U-isJ_UX"
      },
      "source": [
        "imgs = !ls '/content/gdrive/MyDrive/cv/imgs'\n",
        "imgs = ' '.join(imgs).split()\n",
        "faces = []\n",
        "face_boxes_all = []\n",
        "for img in imgs:\n",
        "  print(img)\n",
        "  pxls = open_prepare_image('/content/gdrive/MyDrive/cv/imgs/' + img)\n",
        "  face_boxes = get_face_boxes(pxls)\n",
        "  face_coords = calc_faces_coords(face_boxes)\n",
        "  faces_final = faces_from_coords(face_coords, pxls)\n",
        "  faces = faces + faces_final\n",
        "  face_boxes_all = face_boxes_all + face_boxes\n",
        "\n",
        "faces = np.asarray(faces)\n",
        "face_boxes_all = np.asarray(face_boxes_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGrZZpOB3vkX"
      },
      "source": [
        "Some faces in the images are unidentifiable due to poor image quality. The unsupervised algorithm should still be able to cluster these however we have no way to determine if they were classified correctly, so we remove them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kleMgeT6F0Qy"
      },
      "source": [
        "unknowns = np.argwhere(y=='u').flatten()\n",
        "y_identifiable = np.delete(y, unknowns)\n",
        "faces_identifiable = np.delete(faces, unknowns, axis=0)\n",
        "face_boxes_identifiable = np.delete(face_boxes_all, unknowns, axis=0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDQMgEdHTiFR"
      },
      "source": [
        "faces_float = faces_identifiable.astype('float32')\n",
        "mean, std = faces_float.mean(), faces_float.std()\n",
        "faces_standardized = (faces_float - mean)/std\n",
        "\n",
        "yhat = facenet.predict(faces_standardized)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nxgjm4Pog5a",
        "outputId": "13390ac0-559a-4a61-a96a-afce039222ac"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.cluster import homogeneity_score\n",
        "\n",
        "kmeans = KMeans(n_clusters=len(set(y_identifiable)),init='k-means++',n_init=100, random_state=42).fit_predict(yhat)\n",
        "kmeans"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  5, 12,  2,  2, 15, 16,  9,  6,  8, 12,  3, 13, 15, 16,  6, 13,\n",
              "        9, 12,  8,  3, 15, 12,  8, 16,  9,  3, 13,  6,  3,  9, 15,  2,  3,\n",
              "       15,  9, 12,  6,  7,  3,  2,  3, 12,  9,  6, 15,  7, 15,  3,  3,  3,\n",
              "        2, 14,  3,  7, 15,  2,  2,  3,  7, 15,  3,  9,  5,  7,  3,  7,  5,\n",
              "       13,  2,  2,  3,  4,  9,  7, 13,  2,  2, 13,  2,  2,  2,  5,  5,  2,\n",
              "        2, 13,  2,  2,  5,  5, 13,  2,  2,  5, 14,  2,  5,  5,  2,  2,  3,\n",
              "        2,  3,  4,  2,  3,  3,  4,  2,  4,  3,  2,  2,  3,  4,  2,  3,  4,\n",
              "       11,  7,  9, 14,  5,  1,  0, 10,  6,  8,  5,  2, 14, 14, 10,  0,  7,\n",
              "        5,  9,  1, 11,  8,  6,  5,  5, 14,  5, 11,  1,  7,  6, 14,  9,  0,\n",
              "       10,  8,  5,  5,  7,  3,  2, 11,  9,  5,  1, 10,  6,  0,  5, 14, 14,\n",
              "        8,  5,  3,  9,  7,  0, 11,  1, 14, 10, 14,  6,  5,  8,  5,  5,  3,\n",
              "       14,  9,  7,  0, 11,  1, 14, 10, 14,  6,  5,  8,  5,  5,  3, 14,  9,\n",
              "        1,  7, 14,  0,  5, 11,  8,  5, 10,  5,  5, 14,  1, 11,  0, 10,  7,\n",
              "        5,  8,  5,  2,  5, 14,  5], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2dpzpDG4pQU",
        "outputId": "5550a2c3-eddd-4552-8a28-eaa56bf508a0"
      },
      "source": [
        " homogeneity_score(y_identifiable, kmeans)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7014832262841287"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RruWZ-vFHjN",
        "outputId": "f50b651e-ea77-4281-a261-121ab72d8045"
      },
      "source": [
        "for cluster in range(len(set(y_identifiable))):\n",
        "  print('cluster: ', cluster)\n",
        "  print(y_identifiable[np.where(kmeans == cluster)])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cluster:  0\n",
            "['sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam']\n",
            "cluster:  1\n",
            "['ehaab' 'ehaab' 'ehaab' 'ehaab' 'ehaab' 'ehaab' 'ehaab' 'ehaab']\n",
            "cluster:  2\n",
            "['alec' 'ross' 'ben' 'remi' 'collin' 'amy' 'erin' 'collin' 'jack' 'max'\n",
            " 'jack' 'max' 'max' 'dillon' 'ross' 'ross' 'sam' 'jack' 'max' 'jack' 'max'\n",
            " 'sam' 'alec' 'ross' 'pat' 'julia' 'julia' 'julia' 'pat' 'julia' 'tony'\n",
            " 'collin' 'jack']\n",
            "cluster:  3\n",
            "['sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam'\n",
            " 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam' 'sam'\n",
            " 'sam' 'sam']\n",
            "cluster:  4\n",
            "['benny' 'benny' 'benny' 'benny' 'benny' 'benny']\n",
            "cluster:  5\n",
            "['julia' 'max' 'erin' 'max' 'dillon' 'ross' 'max' 'ross' 'ross' 'ross'\n",
            " 'phillipe' 'tony' 'phillipe' 'jack' 'tony' 'phillipe' 'tony' 'jack'\n",
            " 'phillipe' 'max' 'tony' 'phillipe' 'tony' 'jack' 'phillipe' 'tony' 'jack'\n",
            " 'ehaab' 'tony' 'jack' 'julia' 'erin' 'julia' 'tony' 'jack']\n",
            "cluster:  6\n",
            "['julia' 'julia' 'julia' 'julia' 'julia' 'julia' 'julia' 'julia' 'julia'\n",
            " 'julia' 'julia']\n",
            "cluster:  7\n",
            "['max' 'max' 'max' 'max' 'max' 'max' 'max' 'max' 'max' 'max' 'max' 'max'\n",
            " 'max' 'max' 'max']\n",
            "cluster:  8\n",
            "['pat' 'pat' 'pat' 'pat' 'pat' 'pat' 'pat' 'pat' 'pat' 'pat' 'pat']\n",
            "cluster:  9\n",
            "['erin' 'erin' 'erin' 'erin' 'erin' 'erin' 'erin' 'erin' 'erin' 'erin'\n",
            " 'erin' 'erin' 'erin' 'erin' 'erin']\n",
            "cluster:  10\n",
            "['ross' 'ross' 'ross' 'ross' 'ross' 'ross' 'ross' 'ross']\n",
            "cluster:  11\n",
            "['alec' 'alec' 'alec' 'alec' 'alec' 'alec' 'alec' 'alec']\n",
            "cluster:  12\n",
            "['erin' 'tony' 'tony' 'tony' 'tony' 'tony']\n",
            "cluster:  13\n",
            "['collin' 'collin' 'collin' 'sam' 'sam' 'sam' 'sam' 'sam']\n",
            "cluster:  14\n",
            "['collin' 'max' 'collin' 'benny' 'collin' 'collin' 'benny' 'benny'\n",
            " 'collin' 'collin' 'benny' 'collin' 'collin' 'benny' 'collin' 'benny'\n",
            " 'collin' 'benny']\n",
            "cluster:  15\n",
            "['alec' 'alec' 'alec' 'alec' 'alec' 'alec' 'alec' 'alec' 'alec']\n",
            "cluster:  16\n",
            "['ben' 'ben' 'ben']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLU_FiGE7R6D"
      },
      "source": [
        "It appears as if clusters 2, 5, and 14 have the most incorrect classifications. Lets investigate those further. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m0L3YSr7kfs"
      },
      "source": [
        "cluster2 = np.argwhere(kmeans==2).flatten()\n",
        "cluster5 = np.argwhere(kmeans==5).flatten()\n",
        "cluster14 = np.argwhere(kmeans==14).flatten()\n",
        "\n",
        "def show_cluster(cluster):\n",
        "  plt.figure(figsize = (20,160))\n",
        "\n",
        "  for ix, face in enumerate(cluster):\n",
        "    plt.subplot(55, 5, ix+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(faces_identifiable[face])\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUJiyt-N-hZQ"
      },
      "source": [
        "Cluster2 seems to contain high res images, but many are flipped or rotated. Per the [Facenet paper](https://arxiv.org/pdf/1503.03832.pdf), the algorithm works best when faces are aligned (all in the same rotation). As you'll see below, face alignment can be a tricky problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af78ch_8-YQR"
      },
      "source": [
        "show_cluster(cluster2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L979bxDK_Cw-"
      },
      "source": [
        "Cluster5 contains lots of low-res and rotated images. There isn't much we can do about the low quality, but maybe rotating some of the images to the proper orientation may help. The same goes for Cluster14."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1_couck-ZQ3"
      },
      "source": [
        "show_cluster(cluster5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqvevIDh-ZIj"
      },
      "source": [
        "show_cluster(cluster14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUwLkSoWW3Oc"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nbrs = NearestNeighbors(n_neighbors=19,n_iterations=100, algorithm='ball_tree').fit(yhat)\n",
        "\n",
        "distances, indices = nbrs.kneighbors(yhat)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}